{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq5HP6kqFAnxyrIOQxVycL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antorhasan/BScthesis_bank_image_prediction/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-MnMVhYEk2P",
        "outputId": "8f749b46-5fdc-4fb7-e4a4-8b354aec090a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnrTbeBjCHLG"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import  io\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import models\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray,gray2rgb\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.filters import threshold_otsu"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avnFjmsVBPrG"
      },
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    def __init__(self, image_id, mask_id, root_dir, transform = None):\n",
        "\n",
        "        self.image_id = image_id\n",
        "        self.mask_id = mask_id\n",
        "        self.transform = transform\n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        self.dataset_path = root_dir + \"data_set/\"\n",
        "        self.mask_path = root_dir + \"masks/\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.image_id[index]\n",
        "        image_path = os.path.join(self.dataset_path, image_name)\n",
        "        \n",
        "        img = io.imread(image_path)\n",
        "        img = resize(img, (256, 256))\n",
        "        originalImg = torch.from_numpy(img) # Converting to Torch Format\n",
        "        originalImg = originalImg.permute(2,1,0) # Moving the order to Channel, Width and Height\n",
        "        \n",
        "        mask_name = self.mask_id[index]\n",
        "        mask_path = os.path.join(self.mask_path, mask_name)\n",
        "        \n",
        "        #mask = io.imread(mask_path+\".png\")\n",
        "  \n",
        "        mask = (mask > 125)\n",
        "\n",
        "        mask = resize(mask, (256, 256))\n",
        "        maskIm = torch.from_numpy(mask)\n",
        "       \n",
        "        maskIm = maskIm.unsqueeze_(-1)\n",
        "        maskIm  = maskIm.permute(2, 1, 0)\n",
        "\n",
        "        mask = mask.astype('uint8')\n",
        "        dst = cv2.bitwise_or(img ,img,mask=mask)\n",
        "       \n",
        "        impaintedImg = torch.from_numpy(dst)\n",
        "        impaintedImg  = impaintedImg.permute(2, 1, 0)\n",
        "\n",
        "        if self.transform:\n",
        "            originalImg = self.transform(originalImg)\n",
        " \n",
        "            impaintedImg = self.transform(impaintedImg)\n",
        "\n",
        "        return originalImg,maskIm,impaintedImg"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "FVClIfDFBSfb",
        "outputId": "7501028c-2aed-4ac1-86f3-ee312e32c0a2"
      },
      "source": [
        "train_id = []\n",
        "mask_id = []\n",
        "\n",
        "train_id = os.listdir(\"/content/drive/MyDrive/Dal_Courses/CSCI_6505_Machine_Learning/project/data_set\") # To load image names from the directory\n",
        "mask_id = os.listdir(\"/content/drive/MyDrive/masks\") # Load mask names from directory\n",
        "\n",
        "''' for i in range(1,10244): # 10244 masks in the directory\n",
        "    mask_id_name =  \"mask\" + str(i)\n",
        "    mask_id.append(mask_id_name) '''\n",
        "\n",
        "print(train_id[0])\n",
        "print(mask_id[0])\n",
        "print(asd)\n",
        "\n",
        "my_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor()\n",
        "    #transforms.Normalize(mean=[0.6122, 0.5436, 0.5247], std=[0.2969, 0.3119, 0.3312])\n",
        "])\n",
        "\n",
        "# Ignore for now\n",
        "batch_size = 8\n",
        "learning_rate = 3e-4\n",
        "num_epoch = 100\n",
        "\n",
        "train_dataset = CustomDataSet(image_id=train_id,mask_id = mask_id, root_dir=\"/content/drive/MyDrive/\", transform=my_transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b2b10099bca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/data_set\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# To load image names from the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmask_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/masks\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Load mask names from directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/data_set'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do2AYBhZBVDe"
      },
      "source": [
        "#writer = SummaryWriter(f'/content/drive/MyDrive/')\n",
        "\n",
        "## ------------- To fit single batch\n",
        "\n",
        "#%reload_ext tensorboard\n",
        "#%tensorboard --logdir '/content/drive/MyDrive/'\n",
        "\n",
        "(data,maskIm,targets) = next(iter(train_loader))\n",
        "\n",
        "maskIm = maskIm.type(torch.float)\n",
        "targets = targets.type(torch.float)\n",
        "data = data.type(torch.float)\n",
        "\n",
        "grid_data = torchvision.utils.make_grid(data, nrow=5)\n",
        "plt.imshow(grid_data.permute(2, 1, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tReTJeInBaM8"
      },
      "source": [
        "grid_mask = torchvision.utils.make_grid(maskIm, nrow=5)\n",
        "plt.imshow(grid_mask.permute(2, 1, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa75KOx3BbCm"
      },
      "source": [
        "grid_mask = torchvision.utils.make_grid(targets, nrow=5)\n",
        "plt.imshow(grid_mask.permute(2, 1, 0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}